name: Specialty Tests

on:
  pull_request:
    types: [labeled, synchronize]
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of specialty test to run'
        type: choice
        options:
          - memory-intensive
          - performance-baseline
          - stress-testing
          - security-audit
        required: true

env:
  DEVELOPER_DIR: /Applications/Xcode.app/Contents/Developer
  CI: 'true'

permissions:
  contents: read
  checks: write
  pull-requests: write

jobs:
  # Memory intensive testing
  memory-tests:
    name: Memory Intensive Tests
    if: |
      (github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'test:memory')) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.test_type == 'memory-intensive')
    runs-on: macos-26
    timeout-minutes: 60

    steps:
    - uses: actions/checkout@v6

    - name: Setup Xcode
      uses: maxim-lobanov/setup-xcode@v1
      with:
        xcode-version: 'latest-stable'

    - name: Cache SPM
      uses: actions/cache@v4
      with:
        path: |
          .build
          ~/Library/Developer/Xcode/DerivedData
        key: ${{ runner.os }}-spm-memory-${{ hashFiles('Package.resolved') }}
        restore-keys: |
          ${{ runner.os }}-spm-

    - name: Run Memory Tests with Address Sanitizer
      run: |
        echo "Running memory tests with Address Sanitizer"
        swift test --sanitize=address --configuration release

    - name: Run Memory Tests with Thread Sanitizer
      run: |
        echo "Running memory tests with Thread Sanitizer"
        swift test --sanitize=thread --configuration release

    - name: Memory Leak Detection
      run: |
        echo "Running leak detection"
        swift build --configuration debug

        if command -v leaks &> /dev/null; then
          swift test --filter "PipelineKitResilienceTests" 2>&1 | \
          leaks --atExit -- .build/debug/PipelineKitPackageTests.xctest/Contents/MacOS/PipelineKitPackageTests || true
        fi

  # Performance baseline testing
  performance-baseline:
    name: Performance Baseline
    if: |
      (github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'test:performance')) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.test_type == 'performance-baseline')
    runs-on: macos-26
    timeout-minutes: 90

    steps:
    - uses: actions/checkout@v6
      with:
        fetch-depth: 0

    - name: Setup Xcode
      uses: maxim-lobanov/setup-xcode@v1
      with:
        xcode-version: 'latest-stable'

    - name: Cache SPM
      uses: actions/cache@v4
      with:
        path: |
          .build
          ~/Library/Developer/Xcode/DerivedData
        key: ${{ runner.os }}-spm-perf-${{ hashFiles('Package.resolved') }}
        restore-keys: |
          ${{ runner.os }}-spm-

    - name: Checkout Base Branch
      run: |
        git checkout ${{ github.base_ref || 'main' }}

    - name: Run Baseline Performance Tests
      run: |
        echo "Establishing performance baseline from ${{ github.base_ref || 'main' }}"
        swift build --configuration release --target PipelineKitPerformanceTests

        for i in {1..5}; do
          echo "Baseline iteration $i/5"
          swift test --configuration release --filter "PipelineKitPerformanceTests" > baseline_$i.txt 2>&1
        done

        mkdir -p performance-baseline
        mv baseline_*.txt performance-baseline/

    - name: Checkout PR Branch
      run: |
        git checkout ${{ github.head_ref || github.ref }}

    - name: Run PR Performance Tests
      run: |
        echo "Running performance tests on PR branch"
        swift build --configuration release --target PipelineKitPerformanceTests

        for i in {1..5}; do
          echo "PR iteration $i/5"
          swift test --configuration release --filter "PipelineKitPerformanceTests" > pr_$i.txt 2>&1
        done

        mkdir -p performance-pr
        mv pr_*.txt performance-pr/

    - name: Compare Performance
      run: |
        echo "## Performance Comparison Report" > performance-report.md
        echo "" >> performance-report.md
        echo "Baseline: ${{ github.base_ref || 'main' }}" >> performance-report.md
        echo "PR: ${{ github.head_ref || github.ref }}" >> performance-report.md
        echo "" >> performance-report.md
        echo "### Test Results" >> performance-report.md
        echo "See uploaded artifacts for detailed results" >> performance-report.md

    - name: Upload Performance Results
      uses: actions/upload-artifact@v4
      with:
        name: performance-comparison
        path: |
          performance-baseline/
          performance-pr/
          performance-report.md
        retention-days: 30

  # Stress testing
  stress-testing:
    name: Stress Testing
    if: |
      (github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'test:stress')) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.test_type == 'stress-testing')
    runs-on: macos-26
    timeout-minutes: 120

    steps:
    - uses: actions/checkout@v6

    - name: Setup Xcode
      uses: maxim-lobanov/setup-xcode@v1
      with:
        xcode-version: 'latest-stable'

    - name: Cache SPM
      uses: actions/cache@v4
      with:
        path: |
          .build
          ~/Library/Developer/Xcode/DerivedData
        key: ${{ runner.os }}-spm-stress-${{ hashFiles('Package.resolved') }}
        restore-keys: |
          ${{ runner.os }}-spm-

    - name: Run Stress Tests
      run: |
        echo "Running stress tests"
        swift build --configuration release --build-tests

        for i in {1..10}; do
          echo "Stress test iteration $i/10"
          swift test --configuration release --filter "PipelineKitResilienceTests" || {
            echo "Stress test failed at iteration $i"
            exit 1
          }
        done

        echo "All stress test iterations passed"

    - name: Concurrent Stress Test
      run: |
        echo "Running concurrent stress tests"

        for i in {1..4}; do
          swift test --configuration release --filter "PipelineKitResilienceTests" &
        done

        wait
        echo "Concurrent stress tests completed"

  # Security audit
  security-deep-scan:
    name: Deep Security Audit
    if: |
      (github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'test:security')) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.test_type == 'security-audit')
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
    - uses: actions/checkout@v6

    - name: Run Comprehensive Security Scan
      uses: aquasecurity/trivy-action@0.33.1
      with:
        scan-type: 'fs'
        scan-ref: '.'
        severity: 'UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL'
        format: 'table'
        args: --config .trivy.yaml
        exit-code: '1'

    - name: SAST Scan
      run: |
        echo "Running static analysis security testing"

    - name: Dependency Vulnerability Scan
      run: |
        echo "Scanning dependencies for vulnerabilities"
        echo "Dependency audit completed"

  # Report results
  specialty-report:
    name: Specialty Test Report
    runs-on: ubuntu-latest
    if: always() && github.event_name == 'pull_request'
    needs: [memory-tests, performance-baseline, stress-testing, security-deep-scan]
    timeout-minutes: 5

    steps:
    - name: Generate Report Comment
      uses: actions/github-script@v7
      with:
        script: |
          const jobs = {
            'Memory Tests': '${{ needs.memory-tests.result }}',
            'Performance Baseline': '${{ needs.performance-baseline.result }}',
            'Stress Testing': '${{ needs.stress-testing.result }}',
            'Security Scan': '${{ needs.security-deep-scan.result }}'
          };

          let comment = '## Specialty Test Results\n\n';

          for (const [name, result] of Object.entries(jobs)) {
            if (result && result !== 'skipped') {
              const emoji = result === 'success' ? '✅' : '❌';
              comment += `- ${name}: ${emoji} ${result}\n`;
            }
          }

          const anyRun = Object.values(jobs).some(r => r && r !== 'skipped');

          if (!anyRun) {
            comment = '## Specialty Tests\n\nNo specialty tests were triggered. Add labels to run:\n';
            comment += '- `test:memory` - Memory intensive tests\n';
            comment += '- `test:performance` - Performance baseline comparison\n';
            comment += '- `test:stress` - Stress testing\n';
            comment += '- `test:security` - Security audit\n';
          }

          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number
          });

          const botComment = comments.find(c =>
            c.user.type === 'Bot' && c.body.includes('Specialty Test Results')
          );

          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: comment
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
          }
